{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Изложен метод проектирования устройств подачи ...</td>\n",
       "      <td>УДК 687.053\\n\\nМАТЕМАТИЧЕСКАЯ МОДЕЛЬ РАБОЧЕГО ...</td>\n",
       "      <td>[КРАЕОБМЕТОЧНЫЕ ШВЕЙНЫЕ МАШИНЫ, РАСЧЕТ НИТЕПОД...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Математическая модель рабочего процесса образо...</td>\n",
       "      <td>https://cyberleninka.ru/article/n/matematiches...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В статье представлены исследовательские методы...</td>\n",
       "      <td>/84\\n\\nCivil SecurityTechnology, Vol. 8, 2011,...</td>\n",
       "      <td>[панель из ячеистого бетона, аварийное состоян...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Инженерная безопасность эксплуатации жилых зда...</td>\n",
       "      <td>https://cyberleninka.ru/article/n/inzhenernaya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>Ю.В. Чудодеев\\nИВ РАН\\n\\nМировой финансово-эко...</td>\n",
       "      <td>[КИТАЙ, МИРОВОЙ КРИЗИС, ФИНАНСЫ, ЭКОНОМИКА]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Мировой финансово-экономический кризис как выз...</td>\n",
       "      <td>https://cyberleninka.ru/article/n/mirovoy-fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Представлено исследование актуальной философск...</td>\n",
       "      <td>Методологические проблемы сравнительного анали...</td>\n",
       "      <td>[ТЕХНИЧЕСКАЯ РЕАЛЬНОСТЬ, ПАРАДИГМА, ФИЛОСОФСКИ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Методологические проблемы сравнительного анали...</td>\n",
       "      <td>https://cyberleninka.ru/article/n/metodologich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В статье анализируется суть  как  и понятия. О...</td>\n",
       "      <td>Социокультурный феномен православного монашест...</td>\n",
       "      <td>[СОЦИОКУЛЬТУРНЫЙ ФЕНОМЕН, МОНАШЕСТВО, ПРАВОСЛА...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Социокультурный феномен православного монашества</td>\n",
       "      <td>https://cyberleninka.ru/article/n/sotsiokultur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Изложен метод проектирования устройств подачи ...   \n",
       "1  В статье представлены исследовательские методы...   \n",
       "2                                                      \n",
       "3  Представлено исследование актуальной философск...   \n",
       "4  В статье анализируется суть  как  и понятия. О...   \n",
       "\n",
       "                                             content  \\\n",
       "0  УДК 687.053\\n\\nМАТЕМАТИЧЕСКАЯ МОДЕЛЬ РАБОЧЕГО ...   \n",
       "1  /84\\n\\nCivil SecurityTechnology, Vol. 8, 2011,...   \n",
       "2  Ю.В. Чудодеев\\nИВ РАН\\n\\nМировой финансово-эко...   \n",
       "3  Методологические проблемы сравнительного анали...   \n",
       "4  Социокультурный феномен православного монашест...   \n",
       "\n",
       "                                            keywords summary  \\\n",
       "0  [КРАЕОБМЕТОЧНЫЕ ШВЕЙНЫЕ МАШИНЫ, РАСЧЕТ НИТЕПОД...     NaN   \n",
       "1  [панель из ячеистого бетона, аварийное состоян...     NaN   \n",
       "2        [КИТАЙ, МИРОВОЙ КРИЗИС, ФИНАНСЫ, ЭКОНОМИКА]     NaN   \n",
       "3  [ТЕХНИЧЕСКАЯ РЕАЛЬНОСТЬ, ПАРАДИГМА, ФИЛОСОФСКИ...     NaN   \n",
       "4  [СОЦИОКУЛЬТУРНЫЙ ФЕНОМЕН, МОНАШЕСТВО, ПРАВОСЛА...     NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  Математическая модель рабочего процесса образо...   \n",
       "1  Инженерная безопасность эксплуатации жилых зда...   \n",
       "2  Мировой финансово-экономический кризис как выз...   \n",
       "3  Методологические проблемы сравнительного анали...   \n",
       "4   Социокультурный феномен православного монашества   \n",
       "\n",
       "                                                 url  \n",
       "0  https://cyberleninka.ru/article/n/matematiches...  \n",
       "1  https://cyberleninka.ru/article/n/inzhenernaya...  \n",
       "2  https://cyberleninka.ru/article/n/mirovoy-fina...  \n",
       "3  https://cyberleninka.ru/article/n/metodologich...  \n",
       "4  https://cyberleninka.ru/article/n/sotsiokultur...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt['content_norm'] = data_rt['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X = cv.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17266, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X1 = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD через CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "svd.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1 = svd.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2 = svd.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD через TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd1 = TruncatedSVD(100)\n",
    "svd1.fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1 = svd1.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2 = svd1.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_svd1 = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_X_svd = [X_svd, X_svd1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF через CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=100, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = NMF(100)\n",
    "nmf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1 = nmf.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2  = nmf.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_nmf = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF через TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=100, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf1 = NMF(100)\n",
    "nmf1.fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1 = nmf1.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2  = nmf1.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_nmf1 = [X_text_1, X_text_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    \n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kata/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "dim = 50\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText с нормализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText([text.split() for text in data_rt['content_norm']], size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_ft = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)\n",
    "\n",
    "X_text_ft = [X_text_1_ft, X_text_2_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = [X_text_1_w2v, X_text_2_w2v, X_nmf, X_nmf1, X_svd, X_svd1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for index, pair in enumerate(vectors):\n",
    "    x = pair[0]\n",
    "    y = pair[1]\n",
    "    res = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        ans = cosine_distances(x[i].reshape(1, -1), y[i].reshape(1, -1))[0]\n",
    "        res.append(ans[0])\n",
    "    results[index] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
